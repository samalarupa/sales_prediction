{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAEz1_hSoEx9",
    "outputId": "d6124eb0-b636-4245-b845-fb2f58257422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n",
      "Data Loaded: 42172 rows, 811 products.\n",
      "      Product_Code         ds   y     y_log\n",
      "0               P1 2025-01-07  11  2.484907\n",
      "1               P1 2025-01-14  12  2.564949\n",
      "2               P1 2025-01-21  10  2.397895\n",
      "3               P1 2025-01-28   8  2.197225\n",
      "4               P1 2025-02-04  13  2.639057\n",
      "...            ...        ...  ..       ...\n",
      "42167          P99 2025-12-02   8  2.197225\n",
      "42168          P99 2025-12-09  10  2.397895\n",
      "42169          P99 2025-12-16   7  2.079442\n",
      "42170          P99 2025-12-23   9  2.302585\n",
      "42171          P99 2025-12-30  11  2.484907\n",
      "\n",
      "[42172 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import json\n",
    "from prophet.serialize import model_to_json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONF = {\n",
    "    'input_file': '/content/sales_data_final.csv', # Update path\n",
    "    'forecast_horizon_weeks': 104, # Predict next 2 years\n",
    "    'prophet_model_dir': 'models/prophet_individual',\n",
    "    'global_model_dir': 'models/global_lgbm',\n",
    "    'n_jobs': -1 # Use all cores\n",
    "}\n",
    "\n",
    "for d in [CONF['prophet_model_dir'], CONF['global_model_dir']]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# --- 1. DATA PREPROCESSING ---\n",
    "def load_and_clean_data(filepath):\n",
    "    print(\">>> Loading Data...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Identify date columns (assuming format YYYY-MM-DD or similar in cols)\n",
    "    id_vars = ['Product_Code'] # Add Category if exists\n",
    "    date_cols = [c for c in df.columns if c not in id_vars]\n",
    "\n",
    "    # Melt to Long Format\n",
    "    df_long = df.melt(id_vars=id_vars, value_vars=date_cols, var_name='ds', value_name='y')\n",
    "    df_long['ds'] = pd.to_datetime(df_long['ds'])\n",
    "\n",
    "    # Handle Negative/Zero Sales with Log Transform\n",
    "    # log1p(x) = log(x + 1). This ensures 0 sales -> 0, and no negatives.\n",
    "    df_long['y_log'] = np.log1p(df_long['y'])\n",
    "\n",
    "    df_long = df_long.sort_values(['Product_Code', 'ds']).reset_index(drop=True)\n",
    "    print(f\"Data Loaded: {df_long.shape[0]} rows, {df_long['Product_Code'].nunique()} products.\")\n",
    "    return df_long\n",
    "\n",
    "df_clean = load_and_clean_data(CONF['input_file'])\n",
    "print(df_clean)\n",
    "df_clean.to_csv('./cleaned_dataset_for_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mJuT_rKochP",
    "outputId": "18667fc7-bc82-4b64-9236-bbb34c03a9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Starting Prophet Training for 811 products...\n",
      "    Forecasting Horizon: 104 weeks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Phase 1 Complete. Baseline saved to models/global_lgbm/prophet_full_forecast.parquet\n",
      ">>> Manifest updated with 811 Prophet models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 811 out of 811 | elapsed:  3.2min finished\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from prophet import Prophet\n",
    "from prophet.serialize import model_to_json\n",
    "\n",
    "# --- 2. PROPHET TRAINING & BASELINE GENERATION ---\n",
    "\n",
    "def train_prophet_single(group, product_id, horizon):\n",
    "    \"\"\"\n",
    "    Trains Prophet on log-sales.\n",
    "    Returns: DataFrame containing (ds, yhat_log, Product_Code, type='history'|'future')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare data for Prophet\n",
    "        df_p = group[['ds', 'y_log']].rename(columns={'y_log': 'y'})\n",
    "\n",
    "        # Force yearly seasonality\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=False,\n",
    "                    daily_seasonality=False)\n",
    "\n",
    "        m.fit(df_p)\n",
    "\n",
    "        # ---------------------------------------------------------\n",
    "        # CHANGE 1: Save Model as Native JSON (No Pickle)\n",
    "        # ---------------------------------------------------------\n",
    "        model_path = os.path.join(CONF['prophet_model_dir'], f\"{product_id}.json\")\n",
    "        with open(model_path, 'w') as f:\n",
    "            f.write(model_to_json(m))\n",
    "        # ---------------------------------------------------------\n",
    "\n",
    "        # Create Future Dataframe (History + Future)\n",
    "        future = m.make_future_dataframe(periods=horizon, freq='W')\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        # Clean up result\n",
    "        forecast = forecast[['ds', 'yhat']].rename(columns={'yhat': 'prophet_pred_log'})\n",
    "        forecast['Product_Code'] = product_id\n",
    "\n",
    "        # Mark rows as training or future\n",
    "        max_train_date = df_p['ds'].max()\n",
    "        forecast['split_type'] = forecast['ds'].apply(\n",
    "            lambda x: 'train' if x <= max_train_date else 'future'\n",
    "        )\n",
    "\n",
    "        return forecast\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on {product_id}: {e}\")\n",
    "        # Return empty DF so concat doesn't fail later\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Execution Block ---\n",
    "\n",
    "print(f\">>> Starting Prophet Training for {df_clean['Product_Code'].nunique()} products...\")\n",
    "print(f\"    Forecasting Horizon: {CONF['forecast_horizon_weeks']} weeks\")\n",
    "\n",
    "products = df_clean['Product_Code'].unique()\n",
    "\n",
    "# Run Parallel Training\n",
    "results = Parallel(n_jobs=CONF['n_jobs'], verbose=5)(\n",
    "    delayed(train_prophet_single)(\n",
    "        df_clean[df_clean['Product_Code'] == p],\n",
    "        p,\n",
    "        CONF['forecast_horizon_weeks']\n",
    "    ) for p in products\n",
    ")\n",
    "\n",
    "# Combine all Prophet outputs\n",
    "df_prophet_full = pd.concat(results)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHANGE 2: Save Forecasts as Parquet (No Pickle)\n",
    "# ---------------------------------------------------------\n",
    "forecast_path = os.path.join(CONF['global_model_dir'], \"prophet_full_forecast.parquet\")\n",
    "df_prophet_full.to_parquet(forecast_path, index=False)\n",
    "print(f\">>> Phase 1 Complete. Baseline saved to {forecast_path}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHANGE 3: Auto-Generate Manifest from Success Files\n",
    "# ---------------------------------------------------------\n",
    "# Since Parallel jobs are independent, we scan the directory\n",
    "# afterwards to see which models actually succeeded.\n",
    "successful_models = [\n",
    "    f.replace(\".json\", \"\")\n",
    "    for f in os.listdir(CONF['prophet_model_dir'])\n",
    "    if f.endswith(\".json\")\n",
    "]\n",
    "\n",
    "manifest_update = {\n",
    "    \"prophet_models\": {\n",
    "        \"format\": \"json\",\n",
    "        \"directory\": \"prophet_individual\", # Ensure this matches CONF['prophet_model_dir'] relative path\n",
    "        \"count\": len(successful_models),\n",
    "        \"available_items\": successful_models\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update/Create the manifest file\n",
    "manifest_path = os.path.join(CONF['global_model_dir'], \"model_manifest.json\")\n",
    "\n",
    "# Load existing manifest if it exists (to keep LGBM info), else create new\n",
    "if os.path.exists(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        current_manifest = json.load(f)\n",
    "else:\n",
    "    current_manifest = {}\n",
    "\n",
    "current_manifest.update(manifest_update)\n",
    "\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(current_manifest, f, indent=4)\n",
    "\n",
    "print(f\">>> Manifest updated with {len(successful_models)} Prophet models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4XQHSWGYAzD",
    "outputId": "d4cf36e2-c763-4c9e-9298-9fcbeb4bb934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "us2IEP6Dpgzw",
    "outputId": "f417e83c-eee1-4b39-f8ae-f27eabec2499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Preparing LightGBM Dataset...\n",
      "   Encoder mapping saved to models/global_lgbm/product_encoder.json\n"
     ]
    }
   ],
   "source": [
    "# --- 3. FEATURE ENGINEERING ---\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1. Time Features\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['week'] = df['ds'].dt.isocalendar().week.astype(int)\n",
    "    df['year'] = df['ds'].dt.year\n",
    "\n",
    "    # 2. Cyclical Encoding (Crucial for Seasonality)\n",
    "    # Encodes that Month 12 is close to Month 1\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
    "\n",
    "    return df\n",
    "\n",
    "print(\">>> Preparing LightGBM Dataset...\")\n",
    "\n",
    "# Merge Prophet Predictions with Actuals for Training\n",
    "# We only want the 'train' rows from Prophet for the LightGBM training set\n",
    "df_train_prophet = df_prophet_full[df_prophet_full['split_type'] == 'train']\n",
    "\n",
    "df_ensemble = pd.merge(df_clean, df_train_prophet[['ds', 'Product_Code', 'prophet_pred_log']],\n",
    "                       on=['ds', 'Product_Code'], how='inner')\n",
    "\n",
    "# Apply Feature Engineering\n",
    "df_ensemble = create_features(df_ensemble)\n",
    "\n",
    "# Encode Product Code\n",
    "le = LabelEncoder()\n",
    "df_ensemble['Product_Code_Encoded'] = le.fit_transform(df_ensemble['Product_Code'])\n",
    "\n",
    "# Create a dictionary map: {'Product_A': 0, 'Product_B': 1}\n",
    "# Converting numpy ints to python ints for JSON compatibility\n",
    "encoder_map = {str(k): int(v) for k, v in zip(le.classes_, le.transform(le.classes_))}\n",
    "\n",
    "encoder_path = os.path.join(CONF['global_model_dir'], \"product_encoder.json\")\n",
    "with open(encoder_path, 'w') as f:\n",
    "    json.dump(encoder_map, f)\n",
    "\n",
    "print(f\"   Encoder mapping saved to {encoder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnxJrkqipvPv",
    "outputId": "a7d9baae-4586-4c8c-8c39-2cfd97891a70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training Global LightGBM Model...\n",
      "    Splitting Train/Valid at: 2025-12-09 00:00:00\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttrain's rmse: 0.575582\tvalid's rmse: 0.534947\n",
      "[400]\ttrain's rmse: 0.342573\tvalid's rmse: 0.316822\n",
      "[600]\ttrain's rmse: 0.287996\tvalid's rmse: 0.265123\n",
      "[800]\ttrain's rmse: 0.274281\tvalid's rmse: 0.251882\n",
      "[1000]\ttrain's rmse: 0.269795\tvalid's rmse: 0.247319\n",
      "[1200]\ttrain's rmse: 0.267462\tvalid's rmse: 0.245302\n",
      "[1400]\ttrain's rmse: 0.265826\tvalid's rmse: 0.244261\n",
      "[1600]\ttrain's rmse: 0.264521\tvalid's rmse: 0.243753\n",
      "[1800]\ttrain's rmse: 0.26331\tvalid's rmse: 0.24348\n",
      "Early stopping, best iteration is:\n",
      "[1770]\ttrain's rmse: 0.263498\tvalid's rmse: 0.243418\n",
      ">>> LightGBM Model saved to models/global_lgbm/lgb_global_model.txt\n",
      ">>> Manifest Contract updated.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. GLOBAL LIGHTGBM TRAINING ---\n",
    "\n",
    "print(\">>> Training Global LightGBM Model...\")\n",
    "\n",
    "features = [\n",
    "    'prophet_pred_log',\n",
    "    'Product_Code_Encoded',\n",
    "    'month_sin', 'month_cos',\n",
    "    'week_sin', 'week_cos',\n",
    "    'year'\n",
    "]\n",
    "target = 'y_log' # Train on Log Sales\n",
    "\n",
    "# Robust Time Split\n",
    "# We sort by date. Last 4 weeks = Validation, Rest = Train.\n",
    "unique_dates = sorted(df_ensemble['ds'].unique())\n",
    "split_date = unique_dates[-4] # Reserve last 4 weeks for validation\n",
    "\n",
    "print(f\"    Splitting Train/Valid at: {split_date}\")\n",
    "\n",
    "train_mask = df_ensemble['ds'] <= split_date\n",
    "valid_mask = df_ensemble['ds'] > split_date\n",
    "\n",
    "X_train = df_ensemble.loc[train_mask, features]\n",
    "y_train = df_ensemble.loc[train_mask, target]\n",
    "X_valid = df_ensemble.loc[valid_mask, features]\n",
    "y_valid = df_ensemble.loc[valid_mask, target]\n",
    "\n",
    "# Create Datasets\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dvalid = lgb.Dataset(X_valid, label=y_valid, reference=dtrain)\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.005, # Lower learning rate for better generalization on small data\n",
    "    'num_leaves': 40,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "model_lgb = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[dtrain, dvalid],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "\n",
    "lgbm_filename = \"lgb_global_model.txt\"\n",
    "lgb_model_path = os.path.join(CONF['global_model_dir'], lgbm_filename)\n",
    "model_lgb.save_model(lgb_model_path)\n",
    "print(f\">>> LightGBM Model saved to {lgb_model_path}\")\n",
    "\n",
    "# 2. Update the Manifest Contract\n",
    "manifest_path = os.path.join(CONF['global_model_dir'], \"model_manifest.json\")\n",
    "\n",
    "# Load existing manifest (created in Phase 2)\n",
    "if os.path.exists(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        manifest = json.load(f)\n",
    "else:\n",
    "    manifest = {}\n",
    "\n",
    "# Update Global Model Section\n",
    "manifest['active_global_model'] = lgbm_filename\n",
    "manifest['global_model_config'] = {\n",
    "    \"format\": \"lgbm_text\",\n",
    "    \"expected_features\": len(features),\n",
    "    \"feature_names_ordered\": features,\n",
    "    \"encoder_file\": \"product_encoder.json\"\n",
    "}\n",
    "\n",
    "with open(manifest_path, 'w') as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "print(\">>> Manifest Contract updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o492ecMb0bz",
    "outputId": "92697bbc-3ee5-4e5d-b5c7-832f2db60a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating Final Forecasts...\n",
      "    Predicting for 84344 future rows...\n",
      "✅ Pipeline Finished! Forecasts saved to: final_sales_forecast_2years_safe.csv\n"
     ]
    }
   ],
   "source": [
    "# NEW\n",
    "print(\">>> Generating Final Forecasts...\")\n",
    "\n",
    "# 1. Get the Future Prophet Data\n",
    "df_future = df_prophet_full[df_prophet_full['split_type'] == 'future'].copy()\n",
    "\n",
    "if df_future.empty:\n",
    "    raise ValueError(\"No future dates found in Prophet predictions.\")\n",
    "\n",
    "# 2. Feature Engineering\n",
    "df_future = create_features(df_future)\n",
    "\n",
    "# 3. Encoding (Using the JSON Map we just made)\n",
    "# Logic: Map known products, fill unknown with -1\n",
    "df_future['Product_Code_Encoded'] = df_future['Product_Code'].map(encoder_map).fillna(-1).astype(int)\n",
    "\n",
    "# Filter out unknown products (Safety check)\n",
    "df_future = df_future[df_future['Product_Code_Encoded'] != -1]\n",
    "\n",
    "# 4. LightGBM Prediction\n",
    "print(f\"    Predicting for {df_future.shape[0]} future rows...\")\n",
    "df_future['lgb_pred_log'] = model_lgb.predict(df_future[features])\n",
    "\n",
    "# 5. Inverse Transformation (Log -> Normal Sales)\n",
    "df_future['final_predicted_sales'] = np.expm1(df_future['lgb_pred_log']).clip(lower=0)\n",
    "\n",
    "# 6. Format Final Output\n",
    "final_submission = df_future[['ds', 'Product_Code', 'final_predicted_sales']].copy()\n",
    "final_submission = final_submission.sort_values(['Product_Code', 'ds'])\n",
    "\n",
    "output_path = 'final_sales_forecast_2years_safe.csv'\n",
    "final_submission.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Pipeline Finished! Forecasts saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVjmctE7p_rG",
    "outputId": "79207d5e-9e1d-4f17-b6d8-c6d4f9caf0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Generating Final Forecasts...\n",
      "    Predicting for 84344 future rows...\n",
      "✅ Pipeline Finished! Forecasts saved to: final_sales_forecast_2years_new.csv\n",
      "           ds Product_Code  final_predicted_sales\n",
      "52 2026-01-04           P1              13.034574\n",
      "53 2026-01-11           P1              13.212149\n",
      "54 2026-01-18           P1              10.746154\n",
      "55 2026-01-25           P1               9.667746\n",
      "56 2026-02-01           P1              11.313763\n"
     ]
    }
   ],
   "source": [
    "# OLD --- 5. INFERENCE & FORECAST GENERATION --- Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DU6UgWXhr0EP",
    "outputId": "624ed39f-6eaf-466f-8a04-62f1a4550f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Phase 1: Loading & Preprocessing...\n",
      ">>> Phase 2: Generating Prophet Baselines (Horizon: 104 weeks)...\n",
      ">>> Phase 3: Training Ensemble & Generating Scorecard...\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalid_0's rmse: 0.239147\n",
      "\n",
      "========================================\n",
      "       MODEL PERFORMANCE SCORECARD       \n",
      "========================================\n",
      " Dataset Split Date : 2025-12-09 00:00:00\n",
      "----------------------------------------\n",
      " R-Squared (R²)     : 0.9540  (Fit Quality)\n",
      " MAE                : 1.34  (Avg Error per unit)\n",
      " RMSE (MMSE Proxy)  : 2.22  (Penalizes large errors)\n",
      " WMAPE              : 15.11%\n",
      "----------------------------------------\n",
      " >> ACCURACY        : 84.89% <<\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import Parallel, delayed\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CONF = {\n",
    "    'input_file': '/content/sales_data_final.csv',\n",
    "    'forecast_horizon_weeks': 104,  # 2 Years\n",
    "    'prophet_model_dir': 'models/prophet_individual',\n",
    "    'global_model_dir': 'models/global_lgbm',\n",
    "    'db_connection_str': 'Local-postgres-DB-URL/sales_db', # Update this!\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "for d in [CONF['prophet_model_dir'], CONF['global_model_dir']]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# --- 1. DATA PREPROCESSING ---\n",
    "def load_and_clean_data(filepath):\n",
    "    print(\">>> Phase 1: Loading & Preprocessing...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    id_vars = ['Product_Code']\n",
    "    date_cols = [c for c in df.columns if c not in id_vars]\n",
    "\n",
    "    df_long = df.melt(id_vars=id_vars, value_vars=date_cols, var_name='ds', value_name='y')\n",
    "    df_long['ds'] = pd.to_datetime(df_long['ds'])\n",
    "\n",
    "    # Log Transform for Training (Handle 0s and skew)\n",
    "    df_long['y_log'] = np.log1p(df_long['y'])\n",
    "\n",
    "    df_long = df_long.sort_values(['Product_Code', 'ds']).reset_index(drop=True)\n",
    "    return df_long\n",
    "\n",
    "df_clean = load_and_clean_data(CONF['input_file'])\n",
    "\n",
    "# --- 2. PROPHET FORECAST GENERATION (HISTORY + FUTURE) ---\n",
    "def train_prophet_single(group, product_id, horizon):\n",
    "    try:\n",
    "        df_p = group[['ds', 'y_log']].rename(columns={'y_log': 'y'})\n",
    "\n",
    "        # Hardcoded seasonality for 52-week data\n",
    "        m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)\n",
    "        m.fit(df_p)\n",
    "\n",
    "        future = m.make_future_dataframe(periods=horizon, freq='W')\n",
    "        forecast = m.predict(future)\n",
    "\n",
    "        forecast = forecast[['ds', 'yhat']].rename(columns={'yhat': 'prophet_pred_log'})\n",
    "        forecast['Product_Code'] = product_id\n",
    "\n",
    "        max_train_date = df_p['ds'].max()\n",
    "        forecast['split_type'] = forecast['ds'].apply(lambda x: 'train' if x <= max_train_date else 'future')\n",
    "\n",
    "        return forecast\n",
    "    except Exception as e:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(f\">>> Phase 2: Generating Prophet Baselines (Horizon: {CONF['forecast_horizon_weeks']} weeks)...\")\n",
    "results = Parallel(n_jobs=CONF['n_jobs'], verbose=0)(\n",
    "    delayed(train_prophet_single)(df_clean[df_clean['Product_Code'] == p], p, CONF['forecast_horizon_weeks'])\n",
    "    for p in df_clean['Product_Code'].unique()\n",
    ")\n",
    "df_prophet_full = pd.concat(results)\n",
    "\n",
    "# --- 3. FEATURE ENGINEERING ---\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['week'] = df['ds'].dt.isocalendar().week.astype(int)\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    # Cyclical Features\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
    "    return df\n",
    "\n",
    "# Merge & Prepare\n",
    "df_train_prophet = df_prophet_full[df_prophet_full['split_type'] == 'train']\n",
    "df_ensemble = pd.merge(df_clean, df_train_prophet[['ds', 'Product_Code', 'prophet_pred_log']],\n",
    "                       on=['ds', 'Product_Code'], how='inner')\n",
    "df_ensemble = create_features(df_ensemble)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_ensemble['Product_Code_Encoded'] = le.fit_transform(df_ensemble['Product_Code'])\n",
    "\n",
    "# --- 4. LIGHTGBM TRAINING & SCORECARD ---\n",
    "print(\">>> Phase 3: Training Ensemble & Generating Scorecard...\")\n",
    "\n",
    "features = ['prophet_pred_log', 'Product_Code_Encoded', 'month_sin', 'month_cos', 'week_sin', 'week_cos', 'year']\n",
    "target = 'y_log'\n",
    "\n",
    "# Time-based Split (Last 4 weeks for Validation)\n",
    "unique_dates = sorted(df_ensemble['ds'].unique())\n",
    "split_date = unique_dates[-4]\n",
    "\n",
    "train_mask = df_ensemble['ds'] <= split_date\n",
    "valid_mask = df_ensemble['ds'] > split_date\n",
    "\n",
    "X_train, y_train = df_ensemble.loc[train_mask, features], df_ensemble.loc[train_mask, target]\n",
    "X_valid, y_valid = df_ensemble.loc[valid_mask, features], df_ensemble.loc[valid_mask, target]\n",
    "\n",
    "# Train\n",
    "model_lgb = lgb.train(\n",
    "    {'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.02, 'verbose': -1},\n",
    "    lgb.Dataset(X_train, label=y_train),\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[lgb.Dataset(X_valid, label=y_valid)],\n",
    "    callbacks=[lgb.early_stopping(50)]\n",
    ")\n",
    "\n",
    "# --- SCORECARD GENERATION ---\n",
    "# Predict on Validation Set\n",
    "preds_log = model_lgb.predict(X_valid)\n",
    "preds_real = np.expm1(preds_log).clip(min=0) # Convert back to normal scale\n",
    "actuals_real = np.expm1(y_valid).values\n",
    "\n",
    "# Calculate Metrics\n",
    "mae = mean_absolute_error(actuals_real, preds_real)\n",
    "mse = mean_squared_error(actuals_real, preds_real)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(actuals_real, preds_real)\n",
    "\n",
    "# WMAPE Calculation (Sum of Absolute Errors / Sum of Actuals)\n",
    "wmape = np.sum(np.abs(actuals_real - preds_real)) / np.sum(actuals_real)\n",
    "accuracy = 1.0 - wmape\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"       MODEL PERFORMANCE SCORECARD       \")\n",
    "print(\"=\"*40)\n",
    "print(f\" Dataset Split Date : {split_date}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\" R-Squared (R²)     : {r2:.4f}  (Fit Quality)\")\n",
    "print(f\" MAE                : {mae:.2f}  (Avg Error per unit)\")\n",
    "print(f\" RMSE (MMSE Proxy)  : {rmse:.2f}  (Penalizes large errors)\")\n",
    "print(f\" WMAPE              : {wmape:.2%}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\" >> ACCURACY        : {accuracy:.2%} <<\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8WrNs7pS24uJ",
    "outputId": "6946ed90-d52e-436a-9dc3-fcebbc739163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Phase 4: Production Forecast & Database Export...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5. FINAL FORECAST & DB EXPORT ---\n",
    "print(\">>> Phase 4: Production Forecast & Database Export...\")\n",
    "\n",
    "# Prepare Future Data\n",
    "df_future = df_prophet_full[df_prophet_full['split_type'] == 'future'].copy()\n",
    "df_future = create_features(df_future)\n",
    "df_future['Product_Code_Encoded'] = df_future['Product_Code'].apply(\n",
    "    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    ")\n",
    "df_future = df_future[df_future['Product_Code_Encoded'] != -1]\n",
    "\n",
    "# Predict\n",
    "df_future['predicted_log'] = model_lgb.predict(df_future[features])\n",
    "df_future['predicted_sales'] = np.expm1(df_future['predicted_log']).clip(lower=0)\n",
    "\n",
    "# Format for DB (Clean Columns)\n",
    "df_export = df_future[['ds', 'Product_Code', 'predicted_sales']].rename(\n",
    "    columns={'ds': 'forecast_date', 'Product_Code': 'product_code'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "N42BBeX91icU",
    "outputId": "04be52cc-b914-4dff-9962-9866e28b0442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Phase 6: Saving Forecasts and Metrics to DB (Prisma)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3958590227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     }\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_results_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_export\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.12/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# --- 6. SAVE TO DB USING PRISMA  ---\n",
    "import asyncio\n",
    "print(\">>> Phase 6: Saving Forecasts and Metrics to DB (Prisma)...\")\n",
    "\n",
    "# 1. Update function to ACCEPT the data as arguments\n",
    "async def save_results_to_db(metrics, df_forecasts):\n",
    "    # Connect\n",
    "    db = Prisma()\n",
    "    await db.connect()\n",
    "\n",
    "    try:\n",
    "        # --- A. SAVE METRICS ---\n",
    "        print(f\"   -> Saving Metrics (Accuracy: {metrics['accuracy']:.2%})...\")\n",
    "\n",
    "        await db.modelmetric.create(data={\n",
    "            'training_run_date': pd.Timestamp.now().to_pydatetime(), # Safe conversion\n",
    "            'model_version': 'Hybrid-Prophet-LGBM-v1',\n",
    "            'wmape': float(metrics['wmape']),\n",
    "            'accuracy': float(metrics['accuracy']),\n",
    "            'rmse': float(metrics['rmse']),\n",
    "            'mae': float(metrics['mae']),\n",
    "            'description': 'Automated Weekly Retraining'\n",
    "        })\n",
    "\n",
    "        # --- B. SAVE FORECASTS ---\n",
    "        print(\"   -> Saving Forecasts...\")\n",
    "\n",
    "        forecast_records = []\n",
    "        for _, row in df_forecasts.iterrows():\n",
    "            forecast_records.append({\n",
    "                'product_code': str(row['product_code']),\n",
    "                'forecast_date': row['forecast_date'],\n",
    "                'predicted_sales': float(row['predicted_sales'])\n",
    "            })\n",
    "\n",
    "        await db.salesforecast.delete_many()\n",
    "\n",
    "        await db.salesforecast.create_many(\n",
    "            data=forecast_records,\n",
    "            skip_duplicates=True\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Success! Saved {len(forecast_records)} forecasts and metrics.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ DB Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full error details\n",
    "\n",
    "    finally:\n",
    "        await db.disconnect()\n",
    "\n",
    "# --- 2. EXECUTE WITH ARGUMENTS ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Ensure these variables exist in your main scope!\n",
    "    # If they are inside a function, return them first.\n",
    "    current_metrics = {\n",
    "        'wmape': wmape,\n",
    "        'accuracy': accuracy,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae\n",
    "    }\n",
    "\n",
    "    asyncio.run(save_results_to_db(current_metrics, df_export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kKhbL_j_ebnF"
   },
   "outputs": [],
   "source": [
    "# Also save CSV as backup\n",
    "df_export.to_csv('final_forecast_backup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I78gZ8jM1SU5",
    "outputId": "c09c3c16-1e68-4dcf-8ae4-3df84693426c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x78d387e15040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb.save_model('./trained_lgb_model_for_metrics_new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AaMPTxPUw08q"
   },
   "outputs": [],
   "source": [
    "df_ensemble.to_csv('./latest_df_ensemble.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
